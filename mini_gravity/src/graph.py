from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
from src.tools.file_system import ALL_TOOLS as FS_TOOLS
from src.tools.terminal import ALL_TOOLS as TERM_TOOLS
from langchain_core.messages import SystemMessage
from src.tools.web import WEB_TOOLS
from src.tools.knowledge import search_knowledge
from src.tools_mcp.connector import load_tools as load_postgres
from src.state import AgentState
import asyncio
from dotenv import load_dotenv


load_dotenv()


# 2. Chargement des outils MCP
print("üêò Connexion √† PostgreSQL via MCP...")
db_tools = load_postgres()

# Petit check pour voir si √ßa a march√©
if db_tools:
    print(f"‚úÖ Base de donn√©es connect√©e ! Outils : {[t.name for t in db_tools]}")
else:
    print("‚ö†Ô∏è Attention : Pas de connexion base de donn√©es (V√©rifie ton mot de passe).")

# --- Liste Finale des Outils ---
# On combine tout : Fichiers + Terminal + Web + Base de Donn√©es
ALL_TOOLS = FS_TOOLS + TERM_TOOLS + [search_knowledge] + WEB_TOOLS + db_tools


# --- 1. Initialisation du mod√®le ---
# On utilise gpt-4o pour sa capacit√© de raisonnement
llm = ChatOpenAI(model="gpt-4o", temperature=0)
llm_with_tools = llm.bind_tools(ALL_TOOLS)


# --- 2. D√©finition des N≈ìuds ---
def agent_node(state: AgentState):
    """Le cerveau de l'agent : d√©cide quoi faire."""
    messages = state["messages"]

    # Injection du System Prompt si c'est le d√©but
    if len(messages) == 1:
        if len(messages) == 1:
            system_prompt = SystemMessage(
                content="""Tu es un D√©veloppeur Full-Stack Autonome.

            GESTION DES PROCESSUS (TRES IMPORTANT):
            1. Pour installer ou configurer : Utilise `run_shell_command`.
            2. Pour lancer un SERVEUR (React, NestJS, Python API) : Utilise `start_background_process`.
            - Ne lance JAMAIS un serveur avec run_shell_command sinon tu seras bloqu√©.
            - Donne un nom clair au process (ex: 'api_server').
            3. Apr√®s avoir lanc√© un serveur : Attends quelques secondes et v√©rifie les logs avec `get_process_logs`.
            4. √Ä la fin de ton travail : N'oublie pas de `stop_process`.

            Ta m√©thode : Search -> Learn -> Code -> Start Server -> Check Logs -> Verify.
            """
            )
        messages = [system_prompt] + messages

    # Appel au LLM
    response = llm_with_tools.invoke(messages)

    # Retourne le nouveau message
    return {"messages": [response]}


# N≈ìud pr√©-construit pour ex√©cuter les outils
tool_node = ToolNode(ALL_TOOLS)

# --- 3. Logique de Routing ---


def should_continue(state: AgentState):
    """D√©cide si on arr√™te ou si on appelle un outil."""
    messages = state["messages"]
    last_message = messages[-1]

    # Si le LLM demande un outil -> direction "tools"
    if last_message.tool_calls:
        return "tools"

    # Sinon -> fin
    return END


# --- 4. Construction du Graphe ---

workflow = StateGraph(AgentState)

# √âTAPE CRUCIALE : Ajout des n≈ìuds (C'est ce qui manquait probablement)
workflow.add_node("agent", agent_node)
workflow.add_node("tools", tool_node)

# D√©finition du point de d√©part
workflow.add_edge(START, "agent")

# D√©finition des embranchements
workflow.add_conditional_edges(
    "agent",  # On part de l'agent
    should_continue,  # On v√©rifie la condition
    {
        "tools": "tools",  # Si outil n√©cessaire -> vers n≈ìud 'tools'
        END: END,  # Sinon -> fin
    },
)

# Boucle de retour : Apr√®s un outil, on revient TOUJOURS √† l'agent
workflow.add_edge("tools", "agent")

# Compilation
app = workflow.compile()
